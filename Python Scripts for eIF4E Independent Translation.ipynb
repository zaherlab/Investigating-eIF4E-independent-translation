{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f446021",
   "metadata": {},
   "source": [
    "# Script to get values plotted in Figure 1E\n",
    "\n",
    "Calculates reads normalized to feature length and then normalized again to average CDS coverage for 5'UTRs, 3'UTRs, and introns. Only features that were unique and non-overlapping were included. The corresponding CDS needed to have coverage of at least 128 reads for any feature to be counted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e36272a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob, pathlib\n",
    "\n",
    "#dictionaries to store feature counts for each sample\n",
    "five_prime_utr = {}\n",
    "three_prime_utr = {}\n",
    "intron = {}\n",
    "\n",
    "unique_transcripts = set()\n",
    "with open(\"unique_features.gff\", \"r\") as unique_features:\n",
    "    for line in unique_features:\n",
    "        data = line.strip().split(\"\\t\")\n",
    "        transcript = data[8].split(\"transcript_id \")[1].replace('\"', \"\")\n",
    "        if (data[2] == \"CDS\"):\n",
    "            unique_transcripts.add(transcript)\n",
    "        if (data[2] == \"five_prime_utr\"):\n",
    "            five_prime_utr[transcript] = {}\n",
    "        if (data[2] == \"three_prime_utr\"):\n",
    "            three_prime_utr[transcript] = {}\n",
    "        if (data[2] == \"intron\"):\n",
    "            intron[transcript] = {}\n",
    "        \n",
    "samples = [\"rna_wt_25_1\", \"rna_wt_25_2\", \"rna_wt_25_3\", \n",
    "           \"rna_wt_37_1\", \"rna_wt_37_2\", \"rna_wt_37_3\",\n",
    "           \"rna_ts_25_1\", \"rna_ts_25_2\", \"rna_ts_25_3\",\n",
    "           \"rna_ts_37_1\", \"rna_ts_37_2\", \"rna_ts_37_3\",\n",
    "           \"rpf_wt_25_1\", \"rpf_wt_25_2\",\n",
    "           \"rpf_wt_37_1\", \"rpf_wt_37_2\",\n",
    "           \"rpf_ts_25_1\", \"rpf_ts_25_2\",\n",
    "           \"rpf_ts_37_1\", \"rpf_ts_37_2\"]\n",
    "\n",
    "feature_types = [\"five_prime_utr\", \"CDS\", \"three_prime_utr\", \"intron\"]\n",
    "\n",
    "for sample in samples:\n",
    "    #make dictionaries to store unique transcript information\n",
    "    #element 1 = 5'UTR, 2 = CDS, 3 = 3'UTR, and 4 = intron; same as feature_types above\n",
    "    counts = {}\n",
    "    lengths = {}\n",
    "    for unique_transcript in unique_transcripts:\n",
    "        counts[unique_transcript] = [0, 0, 0, 0]\n",
    "        lengths[unique_transcript] = [0, 0, 0, 0]\n",
    "    \n",
    "    #now go through all the FeatureCounts files to get read counts for each feature\n",
    "    for feature_type in feature_types:\n",
    "        #get the index of the feature type for proper assignment in the counts and lengths dictionaries\n",
    "        feature_index = feature_types.index(feature_type)\n",
    "        #open the corresponding FeatureCounts file\n",
    "        with open(sample + \"_\" + feature_type + \".txt\", \"r\") as in_file:\n",
    "            for line in in_file:\n",
    "                #skip the first two lines of the file\n",
    "                if (\"#\" not in line and \"Geneid\" not in line):\n",
    "                    #grab the transcript id (first column)\n",
    "                    data = line.strip().split(\"\\t\")\n",
    "                    transcript_id = data[0]\n",
    "                    #grab the feature length (6th column) and read count (7th column)\n",
    "                    #assign to their respective dictionary\n",
    "                    feature_length = int(data[5])\n",
    "                    reads = int(data[6])\n",
    "                    lengths[transcript_id][feature_index] = feature_length\n",
    "                    counts[transcript_id][feature_index] = reads\n",
    "\n",
    "    #Once all features are parsed, calculate statistics for features\n",
    "    for unique_transcript in unique_transcripts:\n",
    "        read_counts = counts[unique_transcript]\n",
    "        feature_lengths = lengths[unique_transcript]\n",
    "        cds_counts = read_counts[1]\n",
    "        cds_length = feature_lengths[1]\n",
    "        \n",
    "        if (cds_counts > 0):\n",
    "            cds_average = cds_counts / cds_length        \n",
    "            \n",
    "            #store data for 5'UTR if there are read counts\n",
    "            five_prime_utr_counts = read_counts[0]\n",
    "            five_prime_utr_length = feature_lengths[0]\n",
    "            if (five_prime_utr_length > 0 and five_prime_utr_counts + cds_counts >= 128):\n",
    "                five_prime_utr_average = five_prime_utr_counts / five_prime_utr_length\n",
    "                five_prime_utr[unique_transcript][sample] = five_prime_utr_average / cds_average\n",
    "            \n",
    "            #Same for 3'UTRs\n",
    "            three_prime_utr_counts = read_counts[2]\n",
    "            three_prime_utr_length = feature_lengths[2]\n",
    "            if (three_prime_utr_length > 0 and three_prime_utr_counts + cds_counts >= 128):\n",
    "                three_prime_utr_average = three_prime_utr_counts / three_prime_utr_length\n",
    "                three_prime_utr[unique_transcript][sample] = three_prime_utr_average / cds_average\n",
    "            \n",
    "            #Same for introns\n",
    "            intron_counts = read_counts[3]\n",
    "            intron_length = feature_lengths[3]\n",
    "            if (intron_length > 0 and intron_counts + cds_counts >= 128):\n",
    "                intron_average = intron_counts / intron_length\n",
    "                intron[unique_transcript][sample] = intron_average / cds_average\n",
    "\n",
    "with open(\"five_prime_utr_counts.txt\", \"w\") as out_file:    \n",
    "    #write out samples as header line\n",
    "    out_file.write(\"transcript\\t\")\n",
    "    out_file.write(\"\\t\".join(samples))\n",
    "    out_file.write(\"\\n\")\n",
    "    \n",
    "    #write out values for unique transcripts for each sample  \n",
    "    for transcript, normalized_counts in five_prime_utr.items():\n",
    "        out_file.write(transcript + \"\\t\")\n",
    "        out_file.write(\"\\t\".join(str(value) for value in normalized_counts.values()))\n",
    "        out_file.write(\"\\n\")\n",
    "        \n",
    "with open(\"three_prime_utr_counts.txt\", \"w\") as out_file:\n",
    "    #write out samples as header line\n",
    "    out_file.write(\"transcript\\t\")\n",
    "    out_file.write(\"\\t\".join(samples))\n",
    "    out_file.write(\"\\n\")\n",
    "    \n",
    "    #write out values for unique transcripts for each sample  \n",
    "    for transcript, normalized_counts in three_prime_utr.items():\n",
    "        out_file.write(transcript + \"\\t\")\n",
    "        out_file.write(\"\\t\".join(str(value) for value in normalized_counts.values()))\n",
    "        out_file.write(\"\\n\")\n",
    "        \n",
    "with open(\"intron_counts.txt\", \"w\") as out_file:\n",
    "    #write out samples as header line\n",
    "    out_file.write(\"transcript\\t\")\n",
    "    out_file.write(\"\\t\".join(samples))\n",
    "    out_file.write(\"\\n\")\n",
    "    \n",
    "    #write out values for unique transcripts for each sample  \n",
    "    for transcript, normalized_counts in intron.items():\n",
    "        out_file.write(transcript + \"\\t\")\n",
    "        out_file.write(\"\\t\".join(str(value) for value in normalized_counts.values()))\n",
    "        out_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3494f7b",
   "metadata": {},
   "source": [
    "# Script to get pseudo A-site coordinates from genome mapped ribosome profiling reads\n",
    "\n",
    "Uses the bed converted bam file start and end coordinates. Finds the midpoint of those two coordinates and biases towards the 5' end if read length is odd (eg. for a + strand read, the A-site is calculated as 16 if the midpoint is 16.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f4d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, math\n",
    "\n",
    "#Mapped ribosome profiling reads converted from bam to bed files so we get their 5 and 3' ends\n",
    "for file in glob.glob(\"rpf_*.bed\"):\n",
    "    with open(file, \"r\") as in_file, open(file.replace(\".bed\", \"_coverage.bed\"), \"w\") as out_file:\n",
    "        for line in in_file:\n",
    "            data = line.strip().split(\"\\t\")\n",
    "            #write out the chromosome value to the output file\n",
    "            out_file.write(data[0] + \"\\t\")\n",
    "            #start coordinate\n",
    "            start = int(data[1])\n",
    "            #end coordinate\n",
    "            end = int(data[2])\n",
    "            #Pseudo A-site is the mid point with bias towards the 5' end of the gene\n",
    "            #data[5] is the strand value. If + strand, then bias towards the lower coordinate (the 5' end)\n",
    "            #If - strand, then bias towards the higher coordinate (the 5' end)            \n",
    "            if (data[5] == \"+\"):\n",
    "                start = math.floor((start + end) / 2)\n",
    "            else:\n",
    "                start = math.ceil((start + end) / 2)\n",
    "            #We only care about the A-site coordinate so the end and start coordinate are the same    \n",
    "            end = start\n",
    "            #write out the coordinates for the A-site\n",
    "            out_file.write(str(start) + \"\\t\" + str(end) + \"\\t\")\n",
    "            #copy the rest of the values in the bed file to the output file\n",
    "            out_file.write(data[3] + \"\\t\" + data[4] + \"\\t\" + data[5] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582972d4",
   "metadata": {},
   "source": [
    "# Script to get values plotted in Figure 1F\n",
    "\n",
    "Calculates metagene profiles. Uses coverage from the entire RNA-seq read (Bedtools) or the pseudo A-site for ribosome profiling reads (calculated above), for all unique features. Requires CDS to have at least 128 reads of coverage and for the 5'UTR, CDS, and 3'UTR to be at least 100 nt in length each. Coverage at each position is normalized to average coverage across the whole gene. Coverage at each position in each feature (5'UTR, CDS, 3'UTR) is divided across 100 bins and then averaged across all features of the same type using the SciPy library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c028c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, pathlib, numpy, collections\n",
    "from scipy import stats\n",
    "\n",
    "featureTypes = [\"five_prime_utr\", \"CDS\", \"three_prime_utr\"]\n",
    "\n",
    "#Coverages are in bed files\n",
    "for file in glob.glob(\"*_coverage.bed\"):\n",
    "    #output metagene coverages for each input file\n",
    "    with open(file, \"r\") as in_file, open(file.replace(\"_coverage.bed\", \"_metagene.txt\"), \"w\") as out_file:\n",
    "        transcripts = {}\n",
    "        for line in in_file:\n",
    "            data = line.strip().split(\"\\t\")\n",
    "            featureType = data[2]\n",
    "            if (featureType in featureTypes):\n",
    "                transcript = data[8].split(\"transcript_id \")[1].replace('\"', \"\")\n",
    "                \n",
    "                #Create new entries for transcripts if they haven't already\n",
    "                #Entry is [[5'UTR], [CDS], [3'UTR]]\n",
    "                if (transcript not in transcripts):\n",
    "                    transcripts[transcript] = [collections.deque(), collections.deque(), collections.deque()]\n",
    "\n",
    "                featureIndex = featureTypes.index(featureType)        \n",
    "\n",
    "                strand = data[6]\n",
    "                reads = int(data[10])\n",
    "                \n",
    "                #If strand is positive, add elements to the end to maintain 5' to 3' orientation\n",
    "                #If not, add elements to the beginning to maintain 5' to 3' orientation\n",
    "                if (strand == \"+\"):\n",
    "                    transcripts[transcript][featureIndex].append(reads)\n",
    "                else:\n",
    "                    transcripts[transcript][featureIndex].appendleft(reads)\n",
    "        \n",
    "        meta_five_prime_utr = collections.deque()\n",
    "        meta_cds = collections.deque()\n",
    "        meta_three_prime_utr = collections.deque()\n",
    "\n",
    "        for transcript, counts in transcripts.items():\n",
    "            five_prime_utr = counts[0]\n",
    "            cds = counts[1]\n",
    "            three_prime_utr = counts[2]\n",
    "\n",
    "            #Only calculate for transcripts with at least 128 reads of coverage and 100 nt in length\n",
    "            #Also make sure 5' and 3'UTR are at least 100 nt in length\n",
    "            if (sum(cds) >= 128 and len(cds) >= 100):\n",
    "                if (len(five_prime_utr) >= 100 and len(three_prime_utr) >= 100):\n",
    "                    total_coverage = numpy.array(five_prime_utr + cds + three_prime_utr)\n",
    "                    mean_coverage = numpy.mean(total_coverage)\n",
    "\n",
    "                    five_prime_utr = numpy.array(five_prime_utr)\n",
    "                    cds = numpy.array(cds)\n",
    "                    three_prime_utr = numpy.array(three_prime_utr)\n",
    "                    \n",
    "                    #Divide coverages across 100 equal bins -- our \"metagene\"\n",
    "                    cds_binned = stats.binned_statistic(range(0, len(cds)), cds / mean_coverage, \"mean\", bins = 100)\n",
    "                    five_prime_utr_binned = stats.binned_statistic(range(0, len(five_prime_utr)), \n",
    "                                                                   five_prime_utr / mean_coverage, \"mean\", bins = 100)\n",
    "                    three_prime_utr_binned = stats.binned_statistic(range(0, len(three_prime_utr)), \n",
    "                                                                    three_prime_utr / mean_coverage, \"mean\", bins = 100)\n",
    "\n",
    "                    meta_five_prime_utr.append(five_prime_utr_binned[0])\n",
    "                    meta_cds.append(cds_binned[0])\n",
    "                    meta_three_prime_utr.append(three_prime_utr_binned[0])\n",
    "\n",
    "        meta_five_prime_utr = numpy.array(meta_five_prime_utr)\n",
    "        meta_cds = numpy.array(meta_cds)\n",
    "        meta_three_prime_utr = numpy.array(meta_three_prime_utr)\n",
    "        \n",
    "        #Average coverage at each position in each feature across all transcripts\n",
    "        meta_five_prime_utr = numpy.mean(meta_five_prime_utr, axis=0)                                                \n",
    "        meta_cds = numpy.mean(meta_cds, axis=0)                                                 \n",
    "        meta_three_prime_utr = numpy.mean(meta_three_prime_utr, axis=0)\n",
    "\n",
    "        #Write out values: first 100 are 5'UTR, next 100 are CDS, last 100 are 3'UTR\n",
    "        for value in meta_five_prime_utr:\n",
    "            out_file.write(str(value) + \"\\n\")\n",
    "        for value in meta_cds:\n",
    "            out_file.write(str(value) + \"\\n\")\n",
    "        for value in meta_three_prime_utr:\n",
    "            out_file.write(str(value) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c9433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
